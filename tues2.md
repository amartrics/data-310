# Week Two, Day Two/Three: Tuesday (7/13/21)/Wednesday (7/14/21)
Using the script you produced with the Higgs Dataset, answer the following questions.
1. *Describe the dataset. What type of variable is the target? How many features are being used? How many observations are in the training dataset? How many are used in the validation set?* The Higgs Dataset is comprised of millions of observations (specifically, 11 million observations) collected by the Large Hadron Collider. There are 28 relevant features used in the machine learning practice problem, which relies on a training set of 10,000 observations and a validation or tes\ting set of 1,000 to construct a predictive model. 

2. *How did each of the four models perform (tiny, small, medium and large)? Which of the four models performed the best? Which ones performed the worst? Why in your estimation did certain models perform better? Produce a plot that illustrates and compares all four models.* Out of the four model sizes available, it seems that the tiny model performed the best. There appeared to be the lowest variation between the losses and actuals, and the least amount of stagnation in the validation metric. Meanwhile, the medium and large models performed worst and almost equally as terribly as one another (it is difficult to tell which is performing worse, as the bounds of the graph cut off the training metric lines). In my estimation, the smaller models performed better because it has a smaller number of "learnable parameters," or capacity, that the models had to adapt to or "learn" in order to predict reasonable results. 

![image](https://user-images.githubusercontent.com/70035366/125748781-011322f1-8154-4728-bbf3-4c9267da6e97.png)

3. *Apply regularization, then add a drop out layer and finally combine both regularization with a dropout layer. Produce a plot that illustrates and compares all four models. Why in your estimation did certain models perform better?* Even after regularization, the tiny model still performed best. I would estimate that this is because even with the processes of regularization and dropout aiding in the model's training, the massive sizes of the larger datasets posed too great a challenge. In this case, both L1, which pushed weights exactly towards zero, or L2, a.k.a. weight decay, regularization techniques were used to try to force the model's weights to take on smaller values. I think that since the variety of data across the larger datasets was so vast, regularization did not actually end up simplifying the model.

![image](https://user-images.githubusercontent.com/70035366/125751752-7a89bbfd-e7a6-4218-b947-b6e028ee3b38.png)

4. *What is an overfit model? Why is it important to address it? What are four different ways we have addressed an overfit model thus far?* Overfitting is when a model becomes too familiar with its training dataset, and is thus unable to make real-time, accurate predictions on live data (e.g., validation data). It is important to address in order to ensure the cultivation of a "smart" model that is able to continue learning from new data, not just material it already knows. Thus far, we have addressed overfitting in four different ways: by manipulating the size of the dataset (e.g., adding more training data), reducing the number of learning parameters, adding weight regularization, and adding dropout. 
